{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Automated Grading","text":""},{"location":"#specification-design-and-development-of-a-conversation-genai-powered-dashboard-and-engine-for-automated-grading","title":"Specification, Design and development of a Conversation GenAI powered Dashboard and engine for Automated Grading","text":"<p>Background</p> <p>{width=\"4.169907042869641in\" height=\"3.1259339457567803in\"}</p> <ul> <li> <p>Open-ended questions are a favoured tool among instructors for     assessing student understanding and encouraging critical exploration     of course material.</p> </li> <li> <p>Providing graded feedback for such responses is a time-consuming     task that can lead to overwhelmed instructors and decreased feedback     quality.</p> </li> <li> <p>Many instructors resort to simpler question formats, like     multiple-choice questions, which provide immediate feedback but at     the expense of personalized evaluation and insightful feedback to     improve quality of the student experience.</p> </li> <li> <p>Here, we develop a tool that uses large language models (LLMs),     guided by instructor-defined criteria, to automated grading and     responses to open-ended questions.</p> </li> <li> <p>The tool will deliver grading and rapid personalized feedback,     enabling teachers to evaluate students performance per a provided     rubric as well as for students to quickly test their knowledge and     identify areas for improvement.</p> </li> <li> <p>We provide reference implementations both as a web application and     as a Jupyter Notebook widget that can be used with instructional     coding or math notebooks.</p> </li> <li> <p>With instructor guidance, LLMs hold promise to enhance student     learning outcomes and elevate instructional methodologies.</p> </li> </ul>"},{"location":"#_1","title":"Automated Grading","text":""},{"location":"#approach-and-discussion","title":"Approach and Discussion","text":"<p>{width=\"7.738888888888889in\" height=\"3.209722222222222in\"}</p> <ul> <li> <p>The framework will be capable of defining questions, collecting     student responses, transmitting these responses alongside instructor     expectations to a large language model (LLM), and generating rapid     and personalized feedback for the students.</p> </li> <li> <p>Notably, the entirety of the student-facing workflow can be accessed     as a web application or encapsulated within a Jupyter notebook,     facilitating real-time enhancement of students' understanding of the     course material.</p> </li> <li> <p>The tool can integrate with any application that consumes a JSON     HTTP API, expanding its potential to a wider range of educational     settings.</p> </li> <li> <p>The tool can help small student groups or 'pods' collaboratively     tackle assignments and projects.</p> </li> <li> <p>Human Teaching Assistants, tasked with providing feedback, can     benefit from our tool, as it can streamline grading processes,     reducing potential for attentional errors and freeing up instructors     to deliver more personalized guidance to students.</p> </li> <li> <p>Fully automated student evaluation is challenging both from a     technical perspective and from a human perspective, and thus this     tool is designed not to fully automate grading, but to serve as a     tool that strongly enhances the efficiency of the grading process     benefiting both students and instructors.</p> </li> <li> <p>The too benefits students by providing rapid and personalized     feedback on questions.</p> </li> <li> <p>The tool benefits instructors by helping them to design better     questions and grading criteria, by providing first-pass material for     learning assessments, and by alleviating some of the burden of     providing individualized instruction in large classes.</p> </li> <li> <p>LLMs in general, and Tools like these specifically, are not a     replacement human instructors, but they can nonetheless fill a niche     among education technologies.</p> </li> <li> <p>While LLMs undoubtedly hold immense power and potential it is     crucial to have an in-depth discussion about their ethical     implications, especially in education particularly the potential     biases that LLMs can introduce.</p> </li> <li> <p>These biases could unintentionally touch on sensitive subjects or     unintentionally overlook marginalized groups.</p> </li> <li> <p>Instructors have a role to play by carefully designing their     questions and assessment criteria.</p> </li> <li> <p>Further, students should be made aware of the nature of the system     they are interacting with and its potential to make mistakes or act     on internalized biases.</p> </li> <li> <p>On the other hand, automated systems such as this present an     opportunity to reduce instructors' unconscious biases by evaluating     all students' responses equally and without any explicit     identification.</p> </li> </ul> <p>Effort Approach and Phases</p> <p>This effort uses a mixed methods approach that will be conducted in four main phases.</p> <ul> <li> <p>In the first phase, all the resources related to the functional     dashboard are reviewed in order to identify its functional and     operational requirements.</p> </li> <li> <p>In the second phase, the detailed feature statements and     capabilities of the software are determined by both qualitative     (interview) and quantitative (Delphi) methods.</p> <ul> <li> <p>In this phase, eight people will be interviewed during the     qualitative phase, and thematic analysis will be used to analyse     the data.</p> </li> <li> <p>For the quantitative step, the two- round Delphi technique will     be conducted by the purposeful selection of 21 individuals.</p> </li> </ul> </li> <li> <p>In the third phase,</p> <ul> <li> <p>Deployment of the Spanda AI Software Platform will be first     performed</p> </li> <li> <p>A custom application comprising of a conversationally enabled     GenAI powered dashboard along with the associated model and     generation engine is developed using Python programming language     in an IDE</p> </li> <li> <p>The application will be deployed on a private cloud to ensure     data privacy</p> </li> <li> <p>15 people among faculty members and managers, who are identified     as the users of the dashboard software, are selected to evaluate     the software.</p> </li> <li> <p>Users' satisfaction with the dashboard software is assessed     using a Dashboard Assessment Usability Model.</p> </li> <li> <p>Usability Evaluation Criteria for Dashboards</p> <ul> <li> <p>According to the review of other questionnaires used in     previous studies, the following criteria are identified for     dashboard evaluation: usefulness, operability,     learnability, ease of use, suitability for tasks,     improvement of situational awareness, satisfaction, user     interface, content, and system capabilities</p> </li> <li> <p>Usefulness\u00a0</p> <ul> <li>Usefulness is usually defined as meeting a customer\\'s     needs or providing a competitive advantage with the     product\\'s attributes or benefits. Designers, generally,     aim to deliver useful products. The \"usefulness\"     criterion was used instead of \"effectiveness and     efficiency\" to evaluate the usability of dashboards.</li> </ul> </li> <li> <p>Operability\u00a0</p> <ul> <li>It refers to a user\\'s ability to use and control a     dashboard for performing their tasks. In the present     study, operability included criteria, such as     representation of data in detail, access to various     filters and reports, and ability to correct errors and     support user. The user control is measured under the     \"operability\" criterion.</li> </ul> </li> <li> <p>Learnability\u00a0</p> <ul> <li>Learnability is a quality of software interface that     allows users to quickly become familiar with them and     able to make good use of all their features and     capabilities.</li> </ul> </li> <li> <p>Ease of Use\u00a0</p> <ul> <li>It is a fundamental concept explaining how easily users     can employ a dashboard. This criterion was used for     dashboard evaluation in the EUCS, Health-ITUES, and TAM     questionnaires.</li> </ul> </li> <li> <p>Suitability for Tasks\u00a0</p> <ul> <li>This criterion can help to assess if users can find out     whether a product or system is appropriate for their     needs. It provides support for the users\\' daily     activities and ensures the compatibility and     organization of data on the screen with the user\\'s     tasks.</li> </ul> </li> <li> <p>Improvement of Situational Awareness\u00a0</p> <ul> <li>Situation awareness at a fundamental level is about     understanding what is going on and what might happen     next. The criteria for evaluating situational awareness     were divided into instability representation, complexity     representation, variability representation, arousal     support, concentration support, spare mental capacity     support, and division of attention.</li> </ul> </li> <li> <p>Satisfaction\u00a0</p> <ul> <li>This criterion refers to satisfaction with the features,     capabilities, and ease of use of a dashboard.</li> </ul> </li> <li> <p>User Interface\u00a0</p> <ul> <li>It consists of visual and interactive tools. Visual     tools in a dashboard involve color coding for data     visualization, histogram plots, pie charts, bar graphs,     gauges, data labels, and geographic maps. The     interactive techniques also include customizable     searching, summary view, drill up and drill down, data     ordering and filtering, zoom in and zoom out, and     real-time feature.</li> </ul> </li> <li> <p>Content\u00a0</p> <ul> <li>This criterion involves the quantity and quality of data     displayed by a dashboard. The quantity of displayed data     was measured using two questionnaires (SART and PSSUQ),     while quality was measured using SART. The amount of     displayed data and their compatibility with the users\\'     tasks were also evaluated, and data accuracy, timeliness     (being up-to-date), comprehensiveness, and relevance     were used for measuring data quality.</li> </ul> </li> <li> <p>System Capabilities\u00a0</p> <ul> <li>Evaluation of compatibility is a criterion to assess     software in terms of compatibility with work-related     requirements. The dashboard capabilities are evaluated     to determine how well its compatibility to work-related     processes and how well it satisfies the users\\' data     requirements.</li> </ul> </li> </ul> </li> <li> <p>The collected data will be analysed using descriptive statistics     and data analysis software to suggest feedback and improvement     as well as to assess large-scale deployability</p> </li> </ul> </li> </ul>"},{"location":"#final-product","title":"Final Product","text":"<ul> <li> <p>The final product of this study is a GenAI powered dashboard for     mostly automated grading with human involvement to providing     facilities for instructors grade assignments much more efficiently     based in teacher provided rubric and thus focus on the task of     education.</p> </li> <li> <p>The steps of designing this dashboard can be a basis for developing     better dashboards for evaluating other faculties or even other     universities.</p> </li> </ul>"},{"location":"#definitions-description-of-current-shortcomings-and-needs","title":"Definitions, Description of current shortcomings and needs","text":"<ul> <li> <p>Assessing students' understanding from natural language responses,     however, presents different challenges and has seen significant     evolution</p> </li> <li> <p>LLMs have been shown to outperform domain-specific language models</p> </li> <li> <p>LLMs like GPT-4 could be useful for preliminary grading of     introductory physics assignments, they fall short for     natural-language responses required in comprehensive exam grading.</p> </li> <li> <p>They also fall short on nuanced programming tasks and open-ended     evaluation\u00a0and therefore, in their current state, LLMs should be     treated as a useful but fallible tool, with final assessments still     in the hands of (human) instructors</p> </li> <li> <p>It is also important to consider students perception of AI graders     and how automated graders are deployed to educational settings\u00a0</p> </li> <li> <p>Many comment on the socio-technical dynamics of automated grading,     including the potential for introduction of machine bias</p> </li> <li> <p>To address the evolving needs of grading open-ended responses, this     framework proposes four key enhancements.</p> </li> <li> <p>First, it is specifically designed for open-ended questions, which     are not typically well-served by the simple, rubric-based grading of     most ed-tech tools.</p> </li> <li> <p>Second, the system leverages LLMs to deliver rapid, personalized     feedback for student responses along with a quantitative grade as an     addition that can be controlled.</p> </li> <li> <p>Third, the framework introduces a feedback loop to continually     improve instructor-provided prompts, question suggestions, and     grading criteria.</p> </li> <li> <p>Finally, the tool also integrates with the Jupyter Notebook     environment, extensively utilized in fields such as computer     science, data science, and statistics etc., for extensibility and     modification to suit particular needs.</p> </li> <li> <p>As a data management as well as decision support tool,     conversationally enabled dashboards are one of the most effective     and renowned forms of data objectification.</p> </li> <li> <p>A dashboard can be defined as: \"a tool for visualization that     provides the possibility for acquiring awareness, finding trends,     planning, and real comparisons.</p> </li> <li> <p>These items are repeatedly embodied in a simple and functional user     interface.</p> </li> <li> <p>A dashboard of accumulated data effectively presents multiple     sources and a comprehensive summary of important information that     can be assimilated by faculty members at a glance, queried, directed     to identify good question papers.</p> </li> <li> <p>These dashboards enable organizations to measure, monitor,     characterise and improve the performance of students while driving     continuous instructional improvement.</p> </li> <li> <p>These dashboards build on the foundations of business intelligence,     data integration infrastructure, data science and Generative AI and     are used for monitoring, analysis, and management and decision     support.</p> </li> <li> <p>Developing a grading assistant for a student specific, rubric     included open ended performance evaluation, which is useful for     quickly sharing with the students the information about their     performance in a way that requires minimal effort from instructors     represents enormous value.</p> </li> <li> <p>Generating questions and evaluating the answers presented by     students to generated questions based on lengthy content is     time-consuming and exhausting when required to be done at scale for     faculty members.</p> </li> <li> <p>A dashboard, if designed appropriately with a conversational     interface, can help instructors quickly evaluate answers that are     compliant with course specific instructor provided rubric</p> </li> <li> <p>Such an application enables educators to integrate open-ended     questions into their curriculum without incurring an instructor     labor cost.</p> </li> <li> <p>This allows students to gain rapid, individualized, and     sophisticated feedback, thereby creating a highly effective learning     loop that can enhance the absorption of course materials.</p> </li> <li> <p>It guides students in refining their responses, enhancing their     understanding and application of concepts in each iteration.</p> </li> <li> <p>This feedback is generated by a large language model (LLM), which     circumvents the attentional errors often made by human graders,     particularly when assessing a large volume of assignments.</p> </li> <li> <p>The LLM is capable of delivering intricate responses to students     swiftly</p> </li> </ul> <p>Approach Details</p> <ul> <li> <p>The effort will be carried out by the combined method of consecutive     mixed designs.</p> </li> <li> <p>In a sequential design, the data collection and data analysis of one     component take place after the data collection and data analysis of     the other component and depends on the outcomes of the other     component.</p> </li> <li> <p>Mixed Methods Research combines both closed-ended response data     (quantitative) and open-ended personal data (qualitative).</p> </li> <li> <p>The research environment is the Faculty of BITS. The study and     software development must have obtained an ethical approval and will     be conducted in four phases.</p> </li> </ul>"},{"location":"#phase-one-identification-of-functional-and-non-functional-requirements-of-grading-assistant-via-interviews-and-systematic-research","title":"Phase One: Identification of functional and non-functional requirements of Grading Assistant via interviews and systematic research","text":"<ul> <li> <p>The aim of this phase is to extract the functional parameters of the     faculty, as well as the capabilities of the performance dashboard.</p> </li> <li> <p>In this step, the research and interviews are performed using a     combination of methods. The indicators of performance identified are     divided into five different groups, including education, research,     cultural and student affairs, resource management, and development &amp;     technology, each of which has its own performance indicators.</p> </li> </ul>"},{"location":"#phase-two-requirements-of-the-grading-assistant-dashboard-from-the-perspective-of-end-users","title":"Phase Two: Requirements Of the Grading Assistant Dashboard From The Perspective Of End Users","text":"<ul> <li> <p>This phase is conducted in two steps.</p> </li> <li> <p>In the first step, a qualitative study is conducted to identify the     requirements of the dashboard software.</p> <ul> <li> <p>For this purpose, eight educational group directors and faculty     directors are selected by purposeful sampling for interviews.</p> </li> <li> <p>The average duration of each interview will be 30 minutes.</p> </li> <li> <p>At this stage, after coordinating with the interviewee and     obtaining informed consent, the voice of the interview is     recorded using an electronic recorder, and then its text is     transcribed verbatim in Microsoft Word.</p> </li> <li> <p>The questions of the interview are related to the functional and     non-functional requirements of the dashboard, gathering the     course content as well as the information about the performance     preferences of users.</p> </li> <li> <p>After transcription, the interviews are subjected to code     extraction and then thematic analysis.</p> <ul> <li> <p>Phase 1: Familiarizing yourself with the input data</p> </li> <li> <p>Phase 2: Obtaining initial content along with sample     Questions, Answers and Grades</p> </li> <li> <p>Phase 3: Reviewing Educational content and Rubrics</p> </li> <li> <p>Phase 4: Identifying and grading responses</p> </li> <li> <p>Reviewing generated Grades and Personalized Responses</p> </li> <li> <p>Phase 6: Producing the report themes.</p> </li> </ul> </li> </ul> </li> <li> <p>In the second step, a questionnaire is designed to identify the key     performance indicators of the Grading Assistant using the two- round     Delphi technique.</p> <ul> <li> <p>Twenty individuals are purposefully selected among academic     members, educational group directors, and faculty directors.</p> </li> <li> <p>In the first step of the Delphi technique, a questionnaire with     three-choice questions (disagree, no opinion, and agree) and an     open-ended question at the end of each section are completed, so     people could state if they think anything should be added to the     questionnaire for the second step of the Delphi technique.</p> </li> <li> <p>In the second step of the Delphi Technique, the indicators     proposed are added and subjected to a poll.</p> </li> <li> <p>For data analysis, items with higher than 75% agreement are     accepted, those with an agreement between 50--75% enter the     second round of Delphi, and items with \\&lt;\u200950% agreement are     omitted from the questionnaire.</p> </li> </ul> </li> </ul>"},{"location":"#phase-three-software-development-and-deployment","title":"Phase Three: Software Development and Deployment","text":""},{"location":"#_2","title":"Automated Grading","text":"<ul> <li> <p>For writing the code of this software, the Spanda Platform, Python     and Web Technologies are used.</p> </li> <li> <p>The interface of the software is designed using Html, JQuery, CSS,     and Javascript languages to be run from standard browsers</p> </li> <li> <p>Database software is used for designing tables and managing the     database.</p> </li> <li> <p>Private Cloud connectivity is available</p> </li> </ul>"},{"location":"#phase-fourunderline-evaluation-of-user-satisfaction","title":"Phase Four~~[:]{.underline}~~ Evaluation Of User Satisfaction","text":"<ul> <li> <p>In this phase, 15 of the academic members and managers of the     faculty who are the users of the dashboard software are chosen.</p> </li> <li> <p>In order to evaluate user satisfaction with the dashboard software,     a 20 question Dashboard Assessment Usability Model scored based on a     five-point Likert scale (1 = \\\"completely disagree\\\"; 5 =     \\\"Completely agree\\\") will be used.</p> </li> <li> <p>In addition, two open-ended questions are presented to the     participants so that they can express their viewpoints and     recommendations.</p> </li> <li> <p>This scale will evaluate the dimensions of satisfaction (four     questions), effectiveness (two questions), efficiency (two     questions), operability ( ve questions), learnability (four     questions), user interface aesthetics (one question), appropriate     recognizability (one question), and accessibility (one question).</p> </li> </ul>"},{"location":"#open-ended-questions","title":"Open-ended Questions","text":"<ul> <li> <p>Is there any additional information besides the ones provided here     that you would want to see in the dashboard?</p> </li> <li> <p>Do you have any other comments or suggestions that you would like to     share with us?</p> </li> <li> <p>The validity and reliability of the questionnaire have been     confirmed previously.</p> </li> <li> <p>In the final step, the data are presented in tables using     descriptive statistics such as frequency and percentage. Data     analysis is conducted in Jupyter Notebook software.</p> </li> </ul>"},{"location":"#ethical-considerations","title":"Ethic~~a~~l consider~~a~~tions","text":"<ul> <li> <p>This study has been approved by the University Ethics Committee.</p> </li> <li> <p>The confidentiality and anonymity of participants' information are     strictly observed.</p> </li> <li> <p>During interviews, participants' voices are recorded after obtaining     their written informed consent.</p> </li> <li> <p>Participants' information will not be disclosed in any publication     form, and they will be clearly explained that they have the right to     withdraw from the study at any time.</p> </li> </ul>"},{"location":"#brief-value-discussion","title":"Brief Value Discussion","text":"<ul> <li> <p>This effort aims to design, implement and evaluate the effectiveness     of a GenAI powered dashboard for AI Powered Human Involved grading     and feedback provision to help improve instructor quality of life     and resource management at the faculty level.</p> </li> <li> <p>The steps used for developing this dashboard can provide a basis for     designing better personalized education delivery for improvement of     instructor quality of life and better evaluation of students.</p> </li> <li> <p>Regarding the importance of information integration in organizations     such as universities, it is essential to trace the flow and     dimensions of information.</p> </li> <li> <p>The lack of proper management of information resources can impede     achieving organizational goals</p> </li> <li> <p>Motivating instructor quality of life and quality of student     evaluation via automation engines and graphical dashboards providing     conversation interfaces, language models and generation engines is     possible</p> </li> <li> <p>Eliminating redundant work in different departments, retrieval of     past generated grades and responses and finally, elimination of     duplicate flowing of this information into multiple disparate     organizational databases, which requires spending extra time and     costs to reuse them is possible through this work.</p> </li> <li> <p>The establishment and use of the generated information plays a     significant role in the qualitative development of student     evaluation methods and thus the transformation of the university     into pioneers in the use of AI in education.</p> </li> <li> <p>The information obtained from the information system provides a     powerful management tool in the higher education system.</p> </li> <li> <p>Despite the strengths of this approach, we may face some challenges     while conducting various phases of this research.</p> </li> <li> <p>For example, in phase one, participants may refuse full cooperation     in completing the questionnaire or conducting the interview due to     their busy work schedules.</p> </li> <li> <p>We will try to distribute a considerable number of questionnaires     among users to obviate this challenge.</p> </li> <li> <p>During the implementation phase, the software designed may not be     suitably integrated with other organizational systems, interfering     with information exchange.</p> </li> <li> <p>This challenge will be addressed by writing data processing     pipelines that normalize and integrate data from various sources.</p> </li> </ul>"},{"location":"Fine-Tuning%20Vs.%20RAG/","title":"Fine Tuning Vs. RAG","text":"<p>Knowledge Graphs &amp; LLMs: Fine-Tuning Vs. Retrieval-Augmented Generation</p> <p>What are the limitations of LLMs, and how to overcome them</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Midjourney's idea of a knowledge graph chatbot.</p> <p>This is the second blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase\u00a0[practical uses of these LLMs in conjunction with Neo4j]{.underline}. As part of this project, we will construct and publicly display demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can find the first and third blog posts here:</p> <ul> <li> <p>[Harnessing LLMs With     Neo4j]{.underline}</p> </li> <li> <p>[Multi-Hop Question     Answering]{.underline}</p> </li> </ul> <p>Harnessing Large Language Models with Neo4j</p> <p>Episode 1 --- Exploring Real-World Use Cases</p> <p>medium.com</p> <p>The first wave of hype for Large Language Models (LLMs) came from ChatGPT and similar web-based chatbots, where the models are so good at understanding and generating text that it shocked people, myself included.</p> <p>Many of us logged in and tested its ability to write haikus, motivational letters, or email responses. What became quickly apparent is that LLMs are not only good at generating creative context but also at solving typical natural language processing and other tasks.</p> <p>Shortly after the LLM hype started, people started considering integrating it into their applications. Unfortunately, if you simply develop a wrapper around an LLM API, there is a high chance your application will not be successful as it doesn't provide additional value.</p> <p>One major problem of LLMs is the so-called\u00a0knowledge cutoff. The knowledge cutoff term indicates that LLMs are\u00a0unaware of any events that happened after their training. For example, if you ask ChatGPT about an event in 2023, you will get the following response.</p> <p>{width=\"6.268055555555556in\" height=\"2.292361111111111in\"}</p> <p>ChatGPT's knowledge cutoff date. Image by author.</p> <p>The same problem will occur if you ask an LLM about\u00a0any event not present\u00a0in its training dataset. While the knowledge cutoff date is relevant for any publicly available information, the LLM doesn't have any knowledge about\u00a0private or confidential information\u00a0that might be available even before the knowledge cutoff date.</p> <p>For example, most companies have some confidential information that they don't share publicly but might be interested in having a custom LLM that could answer those questions. On the other hand, a lot of the publicly available information that the LLM is aware of might be already outdated.</p> <p>Therefore, updating and expanding the knowledge of an LLM is highly relevant today.</p> <p>Another problem with LLMs is that they are trained to produce realistic-sounding text, which\u00a0might not be accurate. Some invalid information is more challenging to spot than others. Especially for missing data, it is very probable that the LLM will make up an answer that sounds convincing but is nonetheless wrong instead of admitting that it lacks the base facts in its training.</p> <p>For example, research or court citations might be easier to verify. A week ago, a\u00a0[lawyer got in trouble for blindly believing the court citations ChatGPT produced]{.underline}.</p> <p>Lawyer apologizes for fake court citations from ChatGPT | CNN Business</p> <p>The meteoric rise of ChatGPT is shaking up multiple industries - including law. A lawyer for a man suing Avianca...</p> <p>edition.cnn.com</p> <p>I have also noticed that LLMs will consistently\u00a0produce assertive, yet false information about any sort of IDs\u00a0like the WikiData or other identification numbers.</p> <p>{width=\"6.268055555555556in\" height=\"3.5180555555555557in\"}</p> <p>ChatGPT's hallucinations. Image by author.</p> <p>Since the response by ChatGPT is assertive, you might expect it to be accurate. However, the given WikiData id points to a farm in England. Therefore, you have to be very careful not to blindly believe everything that LLMs produce. Verifying answers or producing more accurate results from LLMs is another big problem that needs to be solved.</p> <p>Of course, LLMs have other problems, like bias, prompt injection, and others. However, we will not talk about them here. Instead, in this blog post, we will present and focus on the\u00a0concepts of fine-tuning and retrieval-augmented LLMs\u00a0and evaluate their pros and cons.</p> <p>Supervised Fine-Tuning of an LLM</p> <p>Explaining how LLMs are trained is beyond the scope of this blog post. Instead, you can watch this\u00a0[incredible video by Andrej Karpathy to catch up on LLMs]{.underline}\u00a0and learn about the different phases of LLM training.</p> <p>By fine-tuning an LLM, we refer to the\u00a0supervised training phase, during which you provide additional question-answer pairs to optimize the performance of the Large Language Model (LLM).</p> <p>Additionally, we have identified two different use cases for fine-tuning an LLM.</p> <p>One use case\u00a0is fine-tuning a model to update and\u00a0expand its internal knowledge.\\ In contrast, the\u00a0other use case\u00a0is focused on fine-tuning a model\u00a0for a specific\u00a0task like text summarization or translating natural language to database queries.</p> <p>First, we will talk about the first use case, where we use fine-tuning techniques to update and expand the internal knowledge of an LLM.</p> <p>{width=\"6.268055555555556in\" height=\"3.1590277777777778in\"}</p> <p>Supervised fine-tuning flow. Image by author. Icons from\u00a0[Flaticons]{.underline}.</p> <p>Usually, you want to avoid pre-training an LLM as the\u00a0cost\u00a0can be upwards of hundreds of thousands and even millions of dollars. A base LLM is pre-trained using a gigantic corpus of text corpus, frequently in the billions or even trillions of tokens.</p> <p>While the\u00a0number of parameters\u00a0of an LLM is vital, it is not the only parameter you should consider when selecting a base LLM. Besides the\u00a0license, you should also consider the\u00a0bias\u00a0and\u00a0toxicity\u00a0of the pre-training dataset and the base LLM.</p> <p>After you have selected the base LLM, you can start the next step of fine-tuning it. The fine-tuning step is relatively cheap regarding computation cost due to available techniques like the\u00a0[LoRa]{.underline}\u00a0and\u00a0[QLoRA]{.underline}.</p> <p>Using LoRA for Efficient Stable Diffusion Fine-Tuning</p> <p>LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal...</p> <p>huggingface.co</p> <p>However,\u00a0constructing a training dataset\u00a0is more complex and can get expensive. If you can not afford a dedicated team of annotators, it seems that the trend is to\u00a0use an LLM to construct a training dataset\u00a0to fine-tune your desired LLM (this is really meta).</p> <p>For example,[\u00a0Stanford's Alpaca training dataset was created using OpenAI's LLMs]{.underline}. The cost to produce 52 thousand training instructions was about 500 dollars, which is relatively cheap.</p> <p>Stanford CRFM</p> <p>We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our...</p> <p>crfm.stanford.edu</p> <p>On the other hand, the\u00a0[Vicuna model was fine-tuned by using the ChatGPT conversations users posted on ShareGPT.com]{.underline}.</p> <p>Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality | LMSYS Org</p> <p>by: The Vicuna Team, Mar 30, 2023 We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on...</p> <p>lmsys.org</p> <p>There is also a relatively\u00a0[fresh project by H2O called WizardLM]{.underline}, which is designed to turn documents into question-answer pairs that can be used to fine-tune an LLM.</p> <p>We haven't found any recent articles describing how to use a knowledge graph to prepare good question-answer pairs that can be used to fine-tune an LLM.</p> <p>This is an area that we plan to explore during the NaLLM project. We have some ideas for utilizing LLMs to construct question-answer pairs from a knowledge graph context.</p> <p>However, there are a lot of unknowns at the moment.\\ For example, can you provide\u00a0two different answers\u00a0to the same question, and the LLM then somehow combines them in its internal knowledge store?</p> <p>Another consideration is that some information in a knowledge graph is not relevant without considering its relationships. Therefore, do we have to pre-define relevant queries, or is there a more generic way to go about it? Or can we use the node-relationship-node patterns representing subject-predicate-object expressions to generate relevant pairs?</p> <p>These are some of the questions we aim to answer in upcoming blog posts.</p> <p>Imagine that you somehow managed to produce a training dataset containing question-answer pairs based on the information stored in your knowledge graph. As a result, the LLM now includes updated knowledge.</p> <p>However, fine-tuning the model didn't solve the knowledge cutoffs problem since it only pushed the knowledge cutoff to a later date.</p> <p>Therefore, we recommend updating the internal knowledge of an LLM through fine-tuning techniques\u00a0only for slowly changing\u00a0or updating data. For example, you could use a fine-tuned model to provide tourist information.</p> <p>However, you would run into troubles the second you would want to include special time-dependent (real-time) or personalized promotions in the responses. Similarly, fine-tuned models are not ideal for analytical workflows where you would ask how many new customers the company gained over the last week.</p> <p>At the moment,\u00a0fine-tuning approaches can help mitigate hallucinations\u00a0but cannot completely eliminate them. One problem is that the LLMs\u00a0do not cite their sources\u00a0when providing answers. Therefore, you have no idea if the answer came from pre-training data, fine-tuning dataset, or was made up by the LLM. Additionally, there might be another possible falsehood source if you use an LLM to create the fine-tuning dataset.</p> <p>Lastly, a fine-tuned model cannot automatically provide different responses\u00a0depending on the user\u00a0making the questions. Likewise, there is no concept of access restrictions, meaning that anybody interacting with the LLM has access to all of its information.</p> <p>Retrieval-Augmented Generation</p> <p>Large language models perform remarkably well in natural language applications like</p> <ul> <li> <p>Text summarization,</p> </li> <li> <p>Extracting relevant information,</p> </li> <li> <p>Disambiguation of entities</p> </li> <li> <p>Translating from one language to another, or even</p> </li> <li> <p>Converting natural language into database queries or scripting code.</p> </li> </ul> <p>Moreover, previously NLP models were most often domain and task-specific, meaning that you would most likely need to train a custom natural language model depending on your use case and domain. However, thanks to the generalization capabilities of LLMs, a single model can be applied to solve various collections of tasks.</p> <p>We have observed quite a strong trend in using retrieval-augmented LLMs, where instead of using LLMs to access its internal knowledge, you use the LLM\u00a0as a natural language interface\u00a0to your company's or private information.</p> <p>{width=\"6.268055555555556in\" height=\"3.1305555555555555in\"}</p> <p>Retrieval-augmented generation. Image by author. Icons from\u00a0[Flaticons]{.underline}.</p> <p>The retrieval augmented approach uses the LLM to generate an answer based on the additionally provided relevant documents from your data source.</p> <p>Therefore, you don't rely on internal knowledge of the LLM to produce answers. Instead, the LLM is used only for extracting relevant information from documents you passed in and summarizing it.</p> <p>ChatGPT plugins</p> <p>openai.com</p> <p>For example, the\u00a0[ChatGPT plugins]{.underline}\u00a0can be thought of as a retrieval-augmented approach to LLM applications. The ChatGPT interface with a browsing plugin enabled allows the LLM to search the internet to access up-to-date information and use it to construct the final answer.</p> <p>{width=\"6.268055555555556in\" height=\"5.6090277777777775in\"}</p> <p>ChatGPT with browsing plugin. Image by author.</p> <p>In this example, ChatGPT was able to answer who won the Oscar for various categories in 2023. But, remember, the cutoff knowledge date for ChatGPT is 2021, so it couldn't know who won the 2023 Oscars from its internal knowledge. Therefore, it accessed external information through the browsing plugin, which allowed it to answer the question with up-to-date information. Those plugins present an integrated augmentation mechanism inside the OpenAI platform.</p> <p>If you have been watching the LLM space, you might have heard of the\u00a0[LangChain library]{.underline}.</p> <p>Getting Started with LangChain: A Beginner's Guide to Building LLM-Powered Applications</p> <p>A LangChain tutorial to build anything with large language models in Python</p> <p>towardsdatascience.com</p> <p>The LangChain library can be used to allow LLMs to access real-time information from various sources like Google Search, vector databases, or knowledge graphs. For example, LangChain has added a\u00a0[Cypher Search chain]{.underline}, which converts natural language questions into a Cypher statement, uses it to retrieve information from the Neo4j database, and constructs a final answer based on the provided information.</p> <p>LangChain has added Cypher Search</p> <p>With the LangChain library, you can conveniently generate Cypher queries, enabling an efficient retrieval of...</p> <p>towardsdatascience.com</p> <p>With the Cypher Search chain, an LLM is not only used to construct a final answer but also to translate a natural language question into a Cypher query.</p> <p>{width=\"6.268055555555556in\" height=\"3.761111111111111in\"}</p> <p>Cypher search in LangChain. Image by author.</p> <p>Another popular library for retrieval-augmented LLM workflows is\u00a0[LlamaIndex (GPT Index)]{.underline}. LlamaIndex is a comprehensive data framework aimed at enhancing the performance of Large Language Models (LLMs) by enabling them to leverage private or custom data.</p> <p>LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)</p> <p>Overview</p> <p>medium.com</p> <p>Firstly, LlamaIndex offers data connectors that facilitate the ingestion of a variety of data sources and formats, encompassing everything from APIs, PDFs, and documents to SQL or graph data.</p> <p>This feature allows for an effortless integration of existing data into the LLM. Secondly, it provides efficient mechanisms to structure the ingested data using indices and graphs, ensuring the data is suitably arranged for use with LLMs. In addition, it includes an advanced retrieval and query interface, which enables users to input an LLM prompt and receive back a context-retrieved, knowledge-augmented output.</p> <p>The idea behind retrieval-augmented LLM applications like ChatGPT Plugins and LangChain is to avoid relying on internal LLM knowledge only to generate answers. Instead, LLMs are used to solve tasks like\u00a0[constructing database queries from natural language]{.underline}\u00a0and constructing answers based on externally provided information or by utilizing plugins/agents for retrieval.</p> <p>The retrieval-augmented approach has some clear advantages over the fine-tuning approach:</p> <ul> <li> <p>The answer can cite its sources of information, which allows you to     validate the information and potentially change or update the     underlying information based on requirements</p> </li> <li> <p>Hallucinations are more unlikely to occur as you don't rely on the     internal knowledge of an LLM to answer the question and only use     information that is provided in the relevant documents</p> </li> <li> <p>Changing, updating, and maintaining the underlying information the     LLM uses is easier as you transform the problem from LLM maintenance     to a database maintenance, querying and context construction problem</p> </li> <li> <p>Answers can be personalized based on the user context, or their     access permission</p> </li> </ul> <p>On the other hand, you should consider the following limitations when using the retrieval-augmented approach:</p> <ul> <li> <p>The answers are only as good as the smart search tool</p> </li> <li> <p>The application needs access to your specific knowledge base, either     that be a database or other data stores</p> </li> <li> <p>Completely disregarding the internal knowledge of the language model     limits the number of questions that can be answered</p> </li> <li> <p>Sometimes LLMs fail to follow instructions, so there is a risk that     the context might be ignored or hallucinations occur if no relevant     answer data is found in the context.</p> </li> </ul> <p>Summary</p> <p>This blog post delves into the limitations of Large Language Models (LLMs), such as</p> <ul> <li> <p>Knowledge cutoff,</p> </li> <li> <p>Hallucinations, and</p> </li> <li> <p>The lack of user customization.</p> </li> </ul> <p>To overcome these, we explored two concepts, namely, fine-tuning and retrieval-augmented use of LLMs.</p> <p>Fine-tuning an LLM\u00a0involves the supervised training phase, where question-answer pairs are provided to optimize the performance of the LLM. This can be used to update and expand the LLM's internal knowledge or fine-tune it for a specific task. However, fine-tuning fails to solve the knowledge cutoff issue as it simply pushes the cutoff to a later date. It also cannot fully eliminate hallucinations. Therefore, we recommend using the fine-tuning approach for slowly changing datasets where some hallucinations are allowed. Since fine-tuning LLMs is relatively new, we are eager to learn more about fine-tuning approaches and best practices.</p> <p>The second approach to overcome the limitations of LLMs is the so-called\u00a0retrieval-augmented generation, where the LLM serves as a natural language interface to access external information, thereby not relying only on its internal knowledge to produce answers. Advantages of the retrieval-augmented approach include source-citing, negligible hallucinations, ease of changing and updating information, and personalization.\\ However, it relies heavily on the intelligent search tool to retrieve relevant information and requires access to the user's knowledge base. Furthermore, it can only answer queries provided it has the information required to address the question.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository.</p> <p>GitHub - neo4j/NaLLM: Repository for the NaLLM project</p> <p>Welcome to the NaLLM project repository, where we are exploring and demonstrating the synergies between Neo4j and Large...</p> <p>github.com</p>"},{"location":"Harnessing%20LLMs%20with%20Neo4j/","title":"Harnessing LLMs with Neo4j","text":"<p>Knowledge Graphs &amp; LLMs: Harnessing Large Language Models with Neo4j</p> <p>Exploring Real-World Use Cases</p> <p>This is the first blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can read the later blog posts here:</p> <ul> <li> <p>[Fine-Tuning vs Retrieval-Augmented     Generation]{.underline}</p> </li> <li> <p>[Multi-Hop Question     Answering]{.underline}</p> </li> </ul> <p>Large Language Models (LLMs) like ChatGPT have taken the world by storm in 2023 due to their ability to understand and generate human-like text. Their capacity to adapt to different conversational contexts, answer questions across a wide range of topics, and even simulate creative writing has revolutionized the way humans and machines interact, sparking a new wave of artificial intelligence applications.</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Steampunk computer wall with a magic mirror operated by ants running in transparent tubes (Midjourney)</p> <p>Thanks to their ability to \"understand\", generate, and refine human-like text, LLMs offer us new methods for working with data. Our team at Neo4j has started a project to\u00a0explore, develop,\u00a0and\u00a0showcase\u00a0practical uses of these LLMs in conjunction with Neo4j.</p> <p>One key aspect of this project is the integration of graph database technology and concepts into the LLM application stack. By doing so, we expect to enhance the\u00a0accuracy, transparency,\u00a0and\u00a0predictability\u00a0of the model output and open up new use-cases both for using LLMs as well as databases.</p> <p>As part of this project, we will construct and publicly display demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute.</p> <p>While we base our project on our current understanding and technology, we fully acknowledge that this is a rapidly advancing field, and future findings may refine our approach. Consequently, our perspective and strategies are subject to change in response to new data and technological progress.</p> <p>Identifying the Real-World Use Cases</p> <p>The initiation phase of our project focused on the identification of real-world use cases, which would form the basis for our upcoming solutions. After thorough research, market analysis, and customer interactions, we've narrowed down\u00a0two initial use cases\u00a0that frequently feature in our conversations with customers.</p> <p>1. Natural Language Interface to a Knowledge Graph</p> <p>Our first use case focuses on developing a natural language interface for knowledge graphs. The goal is to create a user interface that simplifies the process of data selection, querying and processing, making data more accessible and easier to understand.</p> <p>Allowing you to \"talk to your database\".</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Midjourney imagination of a natural language interface for a KG</p> <p>The preferred method for this is a chat-like interface that would\u00a0generate database queries\u00a0based on the user question and the inferred schema of the database.</p> <p>Based on feedback from our users, there is a significant demand for\u00a0natural language responses\u00a0over just citing data and linking to sources. By utilizing LLMs, we can provide these responses, presenting information in a way that mimics natural, human conversation.</p> <p>We're exploring techniques to inform the LLMs about the content of the knowledge graph. This could involve a similarity search on vectorized content passed via context or fine-tuning a model on the knowledge graph itself.</p> <p>{width=\"6.268055555555556in\" height=\"6.060416666666667in\"}</p> <p>Example sequence diagram of a NL interface to KG solution could look like</p> <p>However, while simplicity and comprehensibility are important, so too are the\u00a0accuracy and credibility of information. To ensure this, all responses should include\u00a0links to source data, offering full transparency and traceability.</p> <p>These advancements for LLMs and their integration into knowledge graph interfaces represent an exciting step forward in making complex data more user-friendly and trustworthy.</p> <p>2. Creating a Knowledge Graph From Unstructured Data</p> <p>The second use case showcases the creation of knowledge graphs from a multitude of unstructured data sources, including but not limited to PDFs, HTML pages, and text documents.</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Midjourney has no good concepts of sieves or funnels transforming information :)</p> <p>LLMs interpret various types and meanings in the text, making sense of unstructured data by identifying its inherent structure based on the training data.</p> <p>They can</p> <ul> <li> <p>decipher entities,</p> </li> <li> <p>discern relationships, and</p> </li> <li> <p>eliminate redundancies by recognizing duplicates.</p> </li> </ul> <p>In effect, LLMs can transform a seemingly indistinguishable mass of unstructured text into a well-organized, meaningful knowledge graph of entities and their relationships.</p> <p>{width=\"6.268055555555556in\" height=\"3.8465277777777778in\"}</p> <p>Example sequence diagram of KG creation could look like</p> <p>Interestingly you can guide LLMs with the appropriate prompts to output structured data directly, e.g. as JSON data structures for node- and relationship-lists, that we can feed directly into the graph database.</p> <p>By leveraging LLMs in this way, we can streamline the knowledge graph creation process, improving efficiency and accuracy. Especially the disambiguation helps with the many variants we humans put into our texts for the same entities and relationships just to entertain the reader.</p> <p>As a result, valuable data becomes easier to access, understand, and use for decision-making.</p> <p>Next Steps</p> <p>As the project progresses, our goal will be to develop prototypes for these use cases. We aim to improve the interaction between you, the users, and your connected data using Neo4j and LLMs.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository:\u00a0[https://github.com/neo4j/NaLLM]{.underline}</p>"},{"location":"Knowledge%20Graphs%20From%20Unstructured%20Text/","title":"Knowledge Graphs From Unstructured Text","text":"<p>Construct Knowledge Graphs From Unstructured Text</p> <p>This is the fifth blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we have constructed and publicly displayed demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in a blog post. You can read the previous four blog posts here:</p> <ul> <li> <p>[Harnessing LLMs with     Neo4j]{.underline}</p> </li> <li> <p>[Fine-tuning vs. Retrieval-augmented     generation]{.underline}</p> </li> <li> <p>[Multi-hop Question     Answering]{.underline}</p> </li> <li> <p>[Knowledge Graphs &amp; LLMs: Real-Time Graph     Analytics]{.underline}</p> </li> </ul> <p>This blog post will explore a use case we investigated during our project: extracting information from unstructured data. Organizations have long faced challenges in extracting meaningful insights from unstructured data. Such data encompasses textual content, images, audio, and other non-tabular formats, holding immense potential yet often remaining difficult to use due to its inherent complexity. Our primary focus in this post will be to extract information from unstructured text by converting it into nodes and relationships.</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Illustration from\u00a0[Imagine.art]{.underline}\u00a0of \"extracting information from unstructured data\"</p> <p>Recent years have witnessed significant advancements in natural language processing techniques, revolutionizing the transformation of unstructured data into valuable knowledge. With the emergence of powerful language models like OpenAI's GPT models and leveraging the power of machine learning, the process of converting unstructured text data into structured representations has become more accessible and efficient.</p> <p>One such representation is\u00a0knowledge graphs,\u00a0which offer a robust framework for representing complex relationships and connections among various entities. They provide a structured representation of the data, enabling intuitive querying and exploration of the information contained within. This structured nature allows for advanced semantic analysis, reasoning, and inference, facilitating more accurate and comprehensive decision-making processes.</p> <p>{width=\"5.066666666666666in\" height=\"4.908333333333333in\"}</p> <p>Example of Knowledge Extraction Pipeline</p> <p>We will explore how Large Language Models (LLMs) have simplified the conversion of unstructured data into knowledge graphs, using an approach that utilizes the language skills of LLMs to perform nearly all parts of the process. The process can be divided into three steps:</p> <ol> <li> <p>Extracting nodes and edges</p> </li> <li> <p>Entity disambiguation</p> </li> <li> <p>Importing into Neo4j</p> </li> </ol> <p>Let's walk through each of these steps:</p> <p>1. Extracting nodes and relationships:\u00a0To tackle this problem, we take the simplest possible approach, where we pass the input data to the LLM and let it decide which nodes and relationships to extract. We ask the LLM to return the extracted entities in a specific format, including a name, a type, and properties. This allows us to extract nodes and edges from the input text.</p> <p>However, LLMs have a limitation known as the\u00a0context window\u00a0(between 4 and 16,000 tokens for most LLMs), which can be easily overwhelmed by larger inputs, hindering the processing of such data. To overcome this limitation, we employ a strategy of dividing the input text into smaller, more manageable chunks that fit within the context window.</p> <p>Determining the optimal splitting points for the text is a challenge of its own. To keep things simple, we have chosen to divide the text into chunks of maximum size, maximizing the utilization of the context window per chunk. Additionally, we introduce some overlap from the previous chunk to account for cases where a sentence or description spans across multiple chunks. This approach allows us to extract nodes and edges from each chunk, representing the information contained within it.</p> <p>To maintain consistency in the labeling of different types of entities across chunks, we provide the LLM with a list of node types that were extracted in the previous chunks. Those start forming the extracted \"schema.\" We have observed that this approach enhances the uniformity of the final labels. For example, instead of the LLM generating separate types for \"Company\" and \"Gaming Company,\" it consolidates all types of companies under a \"Company\" label.</p> <p>One\u00a0notable hurdle\u00a0in our approach is the problem of duplicate entities. Since each chunk is processed semi-independently, information about the same entity found in different chunks will create duplicates when we combine the results. Naturally, this issue brings us to our next step.</p> <p>2. Entity disambiguation:\u00a0We now have a set of entities. To address the issue of duplication, we employ LLMs once again. First, we organize the entities into sets based on their type. Subsequently, we provide each set to the LLM, enabling it to merge duplicate entities while simultaneously consolidating their properties. We use LLMs for this since we don't know what name each entity has been given. For example, the initial extraction could have ended up with two nodes: (Alice {name: \"Alice Henderson\"}) and (Alice Henderson {age: 25}). These reference the same entity and should be merged to a single node with both the name and age property. We use LLMs to accomplish this since it's great at quickly understanding which nodes actually reference the same entity.</p> <p>By iteratively performing this procedure for all entity groups, we obtain a structured data set that is ready for further processing.</p> <p>3. Importing the data into Neo4j:\u00a0In the final step of the process, we focus on importing the results we got from the LLM into a Neo4j database. This requires a format that Neo4j can understand. To accomplish this, we parse the generated text from the LLM and transform it into separate CSV files, corresponding to the various node and relationship types. These CSV files are subsequently mapped to a format compatible with the\u00a0[Neo4j Data Importer tool]{.underline}. Through this conversion, we gain the advantage of previewing the data before initiating the import process into a Neo4j database, harnessing the capabilities offered by the Neo4j Importer tool.</p> <p>{width=\"6.268055555555556in\" height=\"3.323611111111111in\"}</p> <p>Overview of the application</p> <p>Putting this all together, we have created an application consisting of three parts: a UI to input a file, a controller that executes the previously explained process, and an LLM that the controller talks to. This demo application can be found\u00a0[here]{.underline}, and the source code can be found on\u00a0[GitHub]{.underline}.</p> <p>We also created a version of this pipeline that works essentially in the same way but with the option to include a schema. This schema works like a filter where the user can restrict which types of nodes and relationships and which properties the LLM should include in its result.</p> <p>NaLLM Graph Construction Demo</p> <p>nallm-experiments.ew.r.appspot.com</p> <p>If you are interested in learning more about generative AI and knowledge graphs, I would suggest taking a look at\u00a0[Neo4j's page about generative AI]{.underline}.</p> <p>Demonstration</p> <p>I tested the application by giving it the Wikipedia page for the\u00a0[James Bond franchise]{.underline}\u00a0and inspected the generated knowledge graph.</p> <p>{width=\"6.268055555555556in\" height=\"3.688888888888889in\"}</p> <p>Example of the resulting graph</p> <p>The provided graph subset showcases the generated graph, which, in my opinion, provides a reasonably accurate depiction of the Wikipedia article. The graph primarily consists of nodes representing books and individuals associated with those books, such as authors and publishers.</p> <p>However, there are a few issues with the graph. For instance, Ian Fleming is labeled as a publisher rather than an author for most of the books he wrote. This discrepancy may be attributed to the difficulty the language model had in comprehending that particular aspect of the Wikipedia article.</p> <p>Another problem is the inclusion of relationships between book nodes and the directors of films with the same titles, instead of creating separate nodes for the movies.</p> <p>Finally, It's worth noting that the LLM appears to be quite literal in its interpretation of relationships, as evidenced by using the relationship type \"used\" to connect the James Bond character with the cars he drove. This literal approach may stem from the article's usage of the verb \"used\" rather than \"drove.\"</p> <p>A full video of the demonstration can be found here:</p> <p>Demo of KG Construction</p> <p>Problems</p> <p>For a demonstration, this approach worked fairly well, and we think it shows that it's possible to use LLMs to create knowledge graphs. However, we acknowledge certain issues need to be addressed within this approach:</p> <ul> <li> <p>Unpredictable output:\u00a0This is inherent to the nature of LLMs. We     do not know how an LLM will format its results. Even if we ask it to     output in a specific format, it might not obey. This might cause     problems when trying to parse what it generates. We saw one instance     of this while chunking the data: Most of the time, the LLM generated     a simple list of nodes and edges, but sometimes the LLM would number     the list. Tools to work around this are starting to be released,     such     as\u00a0[Guardrails]{.underline}\u00a0and\u00a0[OpenAIs     Function API]{.underline}. It's still early in the world of LLM     tooling, so we anticipate that this will not be a problem for long.</p> </li> <li> <p>Speed: This approach is slow and often takes several minutes for     just a single reasonably large web page. There might be a     fundamentally different approach that can make the extraction go     faster.</p> </li> <li> <p>Lack of accountability: There is no way of knowing why the LLM     decided to extract some information from the source documents or if     the information even exists in the source. The data quality of the     resulting knowledge graph is, therefore, much lower than the graph     created by processes not leveraging LLMs.</p> </li> </ul> <p>Summary</p> <p>This blog post explored a use case of Large Language Models with Neo4j to extract insights from unstructured data by converting it into a structured representation in the form of a knowledge graph.</p> <p>We discussed a three-step approach focusing on extracting nodes and relationships, entity disambiguation, and importing the data into Neo4j. By utilizing LLMs, anyone can automate the extraction process and efficiently process large amounts of unstructured data.</p> <p>However, there are challenges\u00a0to address, including unpredictable output formatting, speed limitations, and the lack of accountability. Despite these issues, the combined power of LLMs and Neo4j offers a promising solution for unlocking the hidden value in unstructured data, even for non-technical users.</p>"},{"location":"Multi-Hop/","title":"Multi Hop","text":"<p>Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</p> <p>Retrieve information that spans across multiple documents</p> <p>third blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase practical uses of these\u00a0[LLMs in conjunction with Neo4j]{.underline}. As part of this project, we will construct and publicly display demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can see the previous two blog posts here:</p> <ul> <li> <p>[Harnessing LLMs With     Neo4j]{.underline}</p> </li> <li> <p>[Fine-Tuning vs Retrieval-Augmented     Generation]{.underline}</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Midjourney's idea of an investigative board.</p> <p>In the[\u00a0previous blog post]{.underline}, we learned about the retrieval-augmented approach to overcome the limitations of Large Language Models (LLMs), such as hallucinations and limited knowledge. The idea behind the retrieval-augmented approach is to reference external data at question time and feed it to an LLM to enhance its ability to generate accurate and relevant answers.</p> <p>{width=\"6.268055555555556in\" height=\"3.1305555555555555in\"}</p> <p>Retrieval-augmented approach to LLM applications. Image by author.</p> <p>When a user asks a question, an intelligent search tool looks for relevant information in the provided Knowledge bases. For example, you might have encountered instances of searching for relevant information within PDFs or a company's documentation. Most of those examples use vector similarity search to identify which chunks of text might contain relevant data to answer the user's question accurately. The implementation is relatively straightforward.</p> <p>{width=\"6.268055555555556in\" height=\"3.825in\"}</p> <p>RAG applications using vector similarity search. Image by author.</p> <p>The PDFs or the documentation are first split into multiple chunks of text. Some different strategies include how large the text chunks should be and if there should be any overlap between them. In the next step, vector representations of text chunks are generated by using any of the available text embedding models. That is all the preprocessing needed to perform a vector similarity search at query time. The only step left is to encode the user input as a vector at query time and use cosine or any other similarity to compare the distance between the user input and the embedded text chunks. Most frequently, you will see that the top three most similar documents are returned to provide the context to the LLM to enhance its capability to generate accurate answers. This approach works fairly well when the vector search can produce relevant chunks of text.</p> <p>However, simple vector similarity search might not be sufficient when the LLM needs information from multiple documents or even just multiple chunks to generate an answer.</p> <p>For example, consider the following question:</p> <p>Did any of the former OpenAI employees start their own company?</p> <p>If you think about it, this question can be broken down into two questions.</p> <ul> <li> <p>Who are the former employees of OpenAI?</p> </li> <li> <p>Did any of them start their own company?</p> </li> </ul> <p>{width=\"4.65in\" height=\"4.125in\"}</p> <p>Information spanning across multiple documents. Image by author.</p> <p>Answering these types of questions is a\u00a0multi-hop question-answering task, where a single question can be broken down into multiple sub-questions and can require numerous documents to be provided to the LLM to generate an accurate answer.</p> <p>The above-mentioned workflow of simply chunking and embeddings documents in a database and then using plain vector similarity search might struggle with multi-hop questions due to:</p> <ul> <li> <p>Repeated information in top N documents: The provided documents     are not guaranteed to contain complementary and complete information     needed to answer a question. For example, the top three similar     documents might all mention that\u00a0Shariq\u00a0worked at\u00a0OpenAI\u00a0and     possibly founded a company while completely ignoring all the other     former employees that became founders</p> </li> <li> <p>Missing reference information:\u00a0Depending on the chunk sizes, you     might lose the reference to the entities in the documents. This can     be partially solved by chunk overlaps. However, there are also     examples where the references point to another document, so some     sort of co-reference resolution or other preprocessing would be     needed.</p> </li> <li> <p>Hard to define ideal N number of retrieved documents: Some     questions require more documents to be provided to an LLM to     accurately answer the question, while in other situations, a large     number of provided documents would only increase the noise (and     cost).</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.9256944444444444in\"}</p> <p>An example where the similarity search might return some duplicated information, while other relevant information could be ignored due to a low K number of retrieved information or embedding distance. Image by the author.</p> <p>Therefore, a plain vector similarity search might struggle with multi-hop questions. However, we can employ multiple strategies to attempt to answer multi-hop questions requiring information from various documents.</p> <p>Knowledge Graph as Condensed Information Storage</p> <p>If you are paying close attention to the LLM space, you might have come across the idea of using various techniques to condense information for it to be more easily accessible during query time. For example, you\u00a0[could use an LLM to provide a summary of documents]{.underline}\u00a0and then embed and store the summaries instead of the actual documents. Using this approach, you could remove a lot of noise, get better results, and worry less about prompt token space.</p> <p>Interestingly, you could conduct the contextual summarization at ingestion or\u00a0[perform it during the query time]{.underline}. Contextual compression during query time is interesting as the context is picked that is relevant to the provided question, so it is a bit more guided. However, the heavier the workload during the query time, the worse the expected user latency will be. Therefore, it is recommended to move as much of the workload to ingestion time as possible to improve latency and avoid other runtime issues.</p> <p>The same approach can be applied to\u00a0[summarize conversation history]{.underline}\u00a0to avoid running into\u00a0token limit problems.</p> <p>I haven't seen any articles about combining and summarizing multiple documents as a single record. The problem is probably that there are too many combinations of documents that we could merge and summarize. Therefore, it is perhaps\u00a0too costly to process all the combinations\u00a0of documents at ingestion time.\\ However, a knowledge graph can help here too.</p> <p>The process of extracting structured information in the form of entities and relationships from unstructured text has been around for some time and is better known as\u00a0[the information extraction pipeline]{.underline}. The beauty of combining an information extraction pipeline with knowledge graphs is that you can process each document individually, and the information from different records gets connected when the knowledge graph is constructed or enriched.</p> <p>{width=\"6.268055555555556in\" height=\"4.863888888888889in\"}</p> <p>Extracting entities and relationships from text to construct a knowledge graph. Image by author.</p> <p>The knowledge graph used nodes and relationships to represent data. In this example, the first document provided the information that\u00a0Dario\u00a0and\u00a0Daniela\u00a0used to work at\u00a0OpenAI, while the second document offered information about their\u00a0Anthropic\u00a0startup. Each record was processed individually, yet\u00a0the knowledge graph representation connects the data\u00a0and makes it easy to answer questions spanning across multiple documents.</p> <p>Most of the newer approaches using LLMs to answer multi-hop questions we encountered focus on solving the task at query time. However, we believe that many multi-hop question-answering issues can be solved by preprocessing data before ingestion and connecting it in a knowledge graph. The information extraction pipeline can be\u00a0[performed using LLMs]{.underline}\u00a0or\u00a0[custom text domain models]{.underline}.</p> <p>In order to retrieve information from the knowledge graph at query time, we have to construct an appropriate Cypher statement. Luckily, LLMs are pretty good at translating natural language to Cypher graph-query language.</p> <p>{width=\"6.268055555555556in\" height=\"3.1305555555555555in\"}</p> <p>Using knowledge graphs as part of retrieval-augmented LLM applications. Image by author.</p> <p>In this example, the smart search uses an LLM to generate an appropriate Cypher statement to retrieve relevant information from a knowledge graph. The relevant information is then passed to another LLM call, which uses the original question and the provided information to generate an answer. In practice, you could use different LLMs for generating Cypher statements and answers or use various prompts on a single LLM.</p> <p>Combining Graph and Textual Data</p> <p>Sometimes, you might want to combine textual and graph data to find relevant information. For example, consider the following question:</p> <p>What is the latest news about Prosper Robotics founders?</p> <p>In this example, you might want to identify the Prosper Robotics founders using the knowledge graph structure and retrieve the latest articles mentioning them.</p> <p>{width=\"6.268055555555556in\" height=\"6.795833333333333in\"}</p> <p>Knowledge graph with explicit links between structured information and unstructured text. Image by author.</p> <p>To answer the question about the latest news about Prosper Robotics founders, you would start from the Prosper Robotics node, traverse to its founders, and then retrieve the latest articles mentioning them.</p> <p>A knowledge graph can be used to represent structured information about entities and their relationships, as well as unstructured text as node properties.\u00a0Additionally, you could employ natural language techniques like named entity recognition to connect unstructured information to relevant entities in the knowledge graph, as shown with the\u00a0MENTIONS\u00a0relationship.</p> <p>We believe that the future of retrieval-augmented generation applications is utilizing both structured and unstructured information to generate accurate answers. Therefore, a knowledge graph is a perfect solution because you can store both structured and unstructured data and connect them with explicit relationships, making information more accessible and easier to find.</p> <p>{width=\"6.268055555555556in\" height=\"3.1875in\"}</p> <p>Using Cypher and vector similarity search to retrieve relevant information from a knowledge graph. Image by author.</p> <p>When the knowledge graph contains structured and unstructured data, the smart search tool could utilize Cypher queries or vector similarity search to retrieve relevant information. In some cases, you could also use a combination of the two. For example, you could start with a Cypher query to identify relevant documents and then use vector similarity search to find specific information within those documents.</p> <p>Using Knowledge Graphs in Chain-of-Thought Flow</p> <p>Another very exciting development around LLMs is the so-called\u00a0[chain-of-thought question answering]{.underline}, especially with\u00a0[LLM agents]{.underline}. The idea behind LLM agents is that they can decompose questions into multiple steps, define a plan, and use any of the provided tools. In most cases, the agent tools are APIs or knowledge bases that the agent can access to retrieve additional information. Let's again consider the following question:</p> <p>What is the latest news about Prosper Robotics founders?</p> <p>{width=\"6.268055555555556in\" height=\"6.801388888888889in\"}</p> <p>No explicit links between knowledge graph entities and unstructured text. Image by author.</p> <p>Suppose you don't have explicit connections between articles and entities they mention. The articles and entities could even be in separate databases. In this case, an LLM agent using chain-of-thought flow would be very helpful. First, the agent would decompose the question into sub-questions.</p> <ul> <li> <p>Who are the founders of Prosper Robotics?</p> </li> <li> <p>What is the latest news about them?</p> </li> </ul> <p>Now, an agent could decide which tool to use. Suppose we provide it with a knowledge graph access that it can use to retrieve structured information. Therefore, an agent could choose to retrieve the information about the founders of Prosper Robotics from a knowledge graph. As we already know, the founder of Prosper Robotics is Shariq Hashme. Now that the first question was answered, the agent could rewrite the second subquestion as:</p> <ul> <li>What is the latest news about Shariq Hashme?</li> </ul> <p>The agent could use any of the available tools to answer the subsequent question. The tools can range from knowledge graphs, document or vector databases, various APIs, and more. Having access to structured information allows LLM applications to perform various analytics workflows where aggregation, filtering, or sorting is required. Consider the following questions:</p> <ul> <li> <p>Which company with a solo founder has the highest valuation?</p> </li> <li> <p>Who founded the most companies?</p> </li> </ul> <p>Plain vector similarity search can struggle with these types of analytical questions since it searches through unstructured text data, making it hard to sort or aggregate data. Therefore, a combination of structured and unstructured data is probably the future of retrieval-augmented LLM applications. Additionally, as we have seen, knowledge graphs are also ideal for representing connected information and, consequently, multi-hop queries.</p> <p>While the chain-of-thought is a fascinating development around LLMs as it shows how an LLM can reason, it is not the most user-friendly as the response latency can be high due to multiple LLM calls. However, we are still very excited to understand more about incorporating knowledge graphs into chain-of-thought flows for various use cases.</p> <p>Summary</p> <p>Retrieval-augmented generation applications often require retrieving information from multiple sources to generate accurate answers. While textual summarization can be challenging, representing information in a graph format can offer several advantages.</p> <p>By processing each document separately and connecting them in a knowledge graph, we can construct a structured representation of the information. This approach allows for easier traversal and navigation through interconnected documents, enabling multi-hop reasoning to answer complex queries. Furthermore, constructing the knowledge graph during the ingestion phase reduces the workload during query time, resulting in improved latency.</p> <p>Another advantage of using a knowledge graph is its ability to store both structured and unstructured information. This flexibility makes a\u00a0[knowledge graphs suitable]{.underline}\u00a0for a wide range of language model (LLM) applications, as it can handle various data types and relationships between entities. The graph structure provides a visual representation of the knowledge, facilitating transparency and interpretability for both developers and users.</p> <p>Overall, leveraging knowledge graphs in retrieval-augmented generation applications offers benefits such as improved query efficiency, multi-hop reasoning capabilities, and support for structured and unstructured information.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our\u00a0[GitHub repository]{.underline}.</p> <p>GitHub - neo4j/NaLLM: Repository for the NaLLM project</p> <p>Repository for the NaLLM project. Contribute to neo4j/NaLLM development by creating an account on GitHub.</p> <p>github.com</p>"},{"location":"Real-Time%20Graph%20Analytics/","title":"Real Time Graph Analytics","text":"<p>Knowledge Graphs &amp; LLMs: Real-Time Graph Analytics</p> <p>Understanding data points through the context of their relationships</p> <p>This is the fourth blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can read the previous three blog posts here:</p> <ul> <li> <p>[Harnessing LLMs with     Neo4j]{.underline}</p> </li> <li> <p>[Fine-tuning vs. Retrieval-augmented     generation]{.underline}</p> </li> <li> <p>[Multi-hop Question     Answering]{.underline}</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Graph data analyst as imagined by Midjourney.</p> <p>Large Language Models (LLMs) have significantly changed data accessibility to the average person. Less than a year ago, accessing the company's data required technical skills involving proficiency in numerous dashboarding tools or even diving into the intricacies of a database query language. Yet, with the rise of LLMs like ChatGPT, the wealth of knowledge hidden within private databases or accessible via various APIs is now more readily available than ever with the rise of so-called retrieval-augmented LLM applications.</p> <p>{width=\"6.268055555555556in\" height=\"3.1305555555555555in\"}</p> <p>Retrieval-augmented generation application. Image by author.</p> <p>The idea behind retrieval-augmented applications is to retrieve additional information from various sources to allow the LLM to generate better and more accurate results. It seems OpenAI has also picked up on this trend as they\u00a0[introduced OpenAI\u00a0functions]{.underline}\u00a0recently. The new OpenAI models are trained to use provide parameters to functions (or what other libraries call\u00a0[tools]{.underline}), whose signatures and descriptions are passed in the context, to retrieve additional information at query time if needed.</p> <p>We have observed a strong bias for vector similarity search in retrieval-augmented applications. If you opened Twitter or LinkedIn in the past three months, you might have seen the various \"Chat with your PDFs\" applications. In those examples, the implementation is relatively straightforward. The text is extracted from PDFs, split into chunks if needed, and finally stored in a vector database along with its text embedding representations.</p> <p>The barrier to entry with these types of applications is low, especially if you are dealing with small amounts of data. It is fascinating that so many\u00a0[articles giving the impression that only vector databases are relevant]{.underline}\u00a0for retrieval-augmented applications are published nowadays.</p> <p>While there is immense power in vector similarity-based information retrieval from unstructured text, we believe that structured information has an important role to play in LLM applications.</p> <p>Last time we wrote about\u00a0[multi-hop question answering]{.underline}\u00a0and how knowledge graphs can help solve problems of retrieving information from multiple documents to generate an accurate answer. Additionally, we hinted that vector similarity search is not designed for analytics workflows, where we rely on structured information.</p> <p>Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</p> <p>Retrieve information that spans across multiple documents</p> <p>medium.com</p> <p>For example, questions like:</p> <ul> <li> <p>Who could introduce me to Emil Eifr\u00e9m (CEO of Neo4j)?</p> </li> <li> <p>How is ALOX5 gene related to Crohn's disease?</p> </li> <li> <p>When we have a particular microservice outage, how does it affect     our products?</p> </li> <li> <p>How does a flight delay propagate through the network?</p> </li> <li> <p>Which users can be credited for a social media post virality?</p> </li> </ul> <p>All these questions require highly-connected information to be able to answer the question accurately. For example, to learn who can introduce you to Emil, you need information about relationships between people.</p> <p>On the other hand, you need to map dependencies between your microservices and products in order to evaluate the scale and severity of a particular microservice failure.</p> <p>In this blog post, we will introduce some of the\u00a0frequent use cases of real-time graph analytics\u00a0that you might want to implement into your LLM applications.</p> <p>Finding (Shortest) Paths</p> <p>Relationships are first-class citizens in native graph databases. Although knowledge graphs allow you to perform typical aggregations and filtering to answer questions like \"How many customers did we get this week?\", we will focus more on analytical use cases where traversing the relationships is the main component. One such example is finding the shortest or all possible paths between data points. For example, to answer the question:</p> <p>Who could introduce me to Emil Eifrem?</p> <p>We would have to find the shortest path between myself and Emil Eifrem in the graph.</p> <p>{width=\"6.268055555555556in\" height=\"3.9972222222222222in\"}</p> <p>Single shortest path. Image by author.</p> <p>Another use case where finding the shortest paths in real-time would come in handy is in any sort of transportation, logistics, or routing application. In these applications, you might want to evaluate the top N shortest paths to ensure some fallback plan if something unexpected happens.</p> <p>{width=\"6.268055555555556in\" height=\"3.7104166666666667in\"}</p> <p>Top two shortest paths between stops in Rome. Image by author.</p> <p>This image visualizes the top 2 shortest paths between two stops in Rome. Such shortest paths could be optimized for distance, time, cost, or a combination.</p> <p>Another domain where finding paths between data points in your LLM applications is the biomedical domain. In the biomedical domain, you are dealing with genes, proteins, diseases, drugs, and more. What's perhaps more important is that these entities do not exist in isolation but have complex, often multilayered relationships with each other.</p> <p>For instance, a gene may be associated with multiple diseases, a protein may interact with numerous other proteins, a disease might be treatable by a variety of drugs, and a drug could have multiple effects on different genes and proteins.</p> <p>Given the staggering amount of biomedical data available, the number of potential relationships between these data points is enormous and is a great candidate to be represented as a knowledge graph.</p> <p>{width=\"6.268055555555556in\" height=\"5.9743055555555555in\"}</p> <p>All shortest paths between Crohn's disease and ALOX5 gene using\u00a0[Hetionet dataset]{.underline}. Image by author.</p> <p>Biomedical knowledge graph can support LLM applications where users are be interested in answering questions like</p> <p>How is ALOX5 gene related to Crohn's disease?</p> <p>While most LLM applications we see today generate answers as a natural language, there is also an excellent opportunity for returning responses in the form of line, bar, or even network visualizations. Often the LLM can even return the configuration structure needed for the charting libraries.</p> <p>Information Propagating Through Network</p> <p>Another strong knowledge graph fit is domains with networks of dependencies. For example, you could have a knowledge graph containing the complete\u00a0[microservice architecture of your system]{.underline}. Such a knowledge graph would allow you to power a DevOps chatbot that would enable you to evaluate the architecture in real-time and perform what-if analysis.</p> <p>{width=\"6.268055555555556in\" height=\"3.839583333333333in\"}</p> <p>Microservices &amp; People graph. Screenshot from\u00a0[https://www.youtube.com/watch?v=_qakAUjXiek&amp;t=2517s]{.underline}</p> <p>Also Rhys Evans presented how the\u00a0[Financial Times manages their infrastructure as a graph]{.underline}.</p> <p>Another domain that comes to mind is the supply chain.</p> <p>{width=\"6.268055555555556in\" height=\"3.359722222222222in\"}</p> <p>Image from\u00a0[https://neo4j.com/blog/graphs-in-automotive-and-manufacturing/]{.underline}.</p> <p>Incorporating\u00a0[supply chain data into knowledge graphs]{.underline}\u00a0can significantly enhance the capabilities of large language applications. This approach allows us to structure complex supply chain information into nodes and relationships, thereby generating a holistic picture of how materials, components, and products flow from suppliers to customers. The inherent interconnections and dependencies become evident and analyzable.</p> <p>For language applications, this enables deeper context understanding and knowledge generation. For instance, an AI model like ChatGPT can leverage this data structure to produce more accurate and insightful responses about supply chain scenarios, disruptions, or management strategies. It could comprehend and explain the ripple effects of a shortage of a certain component, predict potential bottlenecks, or suggest optimization strategies.</p> <p>By aligning the intricacies of supply chain dynamics with the cognitive abilities of AI, we can bolster the functionality and value of large language applications in a multitude of industrial and commercial contexts.</p> <p>Social Network Analysis and Data Science</p> <p>What if your company chatbot went beyond documentation and helped deliver insights and recommendations as part of the people analytics?</p> <p>{width=\"6.268055555555556in\" height=\"4.699305555555555in\"}</p> <p>Image from\u00a0[https://neo4j.com/blog/neo4j-critical-aspect-human-capital-management-hcm/]{.underline}.</p> <p>[Knowledge graphs in HCM]{.underline}\u00a0can serve as an invaluable tool in driving people analytics within a company, primarily by creating a robust, interconnected system of information that allows for a deep, holistic understanding of employee behavior, skills, competencies, interactions, and performance. Essentially, a knowledge graph captures and links complex employee data --- including demographic information, role history, project involvement, performance indicators, and skillsets --- allowing for multi-faceted analysis.</p> <p>This combination of connected data and ML powered tools enable human resources and team leaders to uncover hidden patterns, identify high-potential individuals, predict future performance, assess skill gaps, and inform training needs, thereby fueling data-driven decision-making. By leveraging a knowledge graph, companies can streamline talent management and development processes, enhancing overall organizational effectiveness and fostering a culture of continuous learning and improvement.</p> <p>Incorporating a chatbot interface into this knowledge graph-driven people analytics system could revolutionize the way companies approach HR and talent management. Here's how:</p> <p>User-Friendly Access to Complex Data</p> <p>A chatbot interface provides an intuitive, conversational manner for users to interact with complex datasets. Employees, managers, or HR staff wouldn't need to understand intricate databases or analytics tools; they could simply ask the chatbot questions about employee performance, skills, or team dynamics.</p> <p>The chatbot, equipped with natural language processing capabilities, would interpret the question, retrieve the relevant information from the knowledge graph, and deliver the response in an understandable format.</p> <p>Real-Time Insights</p> <p>The chatbot interface could offer immediate access to data insights, enabling timely decisions.</p> <p>If a manager wanted to know how many projects are in the pipeline and which people are a good fit and available for a specific project, they could ask the chatbot and get an answer in real-time rather than waiting for a comprehensive report.</p> <p>Scalable Training and Support</p> <p>The chatbot could provide individualized support to employees, answering questions about company policies, procedures, or career development opportunities.</p> <p>It could even deliver personalized training recommendations (and perhaps the actual training itself) based on an individual's role, skills, and career goals. This would democratize access to learning and development resources, making it easier for employees to upskill or reskill.</p> <p>Predictive Analysis</p> <p>Advanced AI chatbots could analyze patterns and trends from the knowledge graph to make predictions, such as which employees might be at risk of leaving the company or what skills may be in demand in the future. These predictive analytics capabilities could help companies be proactive rather than reactive in their HR strategies.</p> <p>In essence, integrating a chatbot interface with a knowledge graph-driven people analytics system would make complex employee data more accessible, actionable, and useful to all members of an organization. It would be a game-changer in talent management and development, driving a more data-informed, proactive, and personalized approach to HR.</p> <p>Summary</p> <p>In conclusion, as we traverse deeper into the era of large language models, we must keep in mind the\u00a0enormous potential of knowledge graphs in structuring, organizing, and retrieving information\u00a0in these applications. The combination of structured and unstructured data retrieval paves the way for more accurate, reliable, and impactful results, extending beyond natural language answers into the realm of visually represented information.</p> <p>Despite the popularity of vector similarity-based data retrieval (recall), we should not underestimate the role of structured information and the immense value it brings to LLM applications. Whether it's finding the shortest paths, understanding complex biomedical relationships, analyzing supply chain scenarios, or revolutionizing HR with people analytics, the applications of knowledge graphs are vast and profound. We believe the future of LLM-based applications is the combination of vector similarity search approach coupled with database query languages such as Cypher.</p> <p>Through this blog post, we've explored some exciting real-time graph analytics use cases that could be implemented into your LLM applications. This is only the beginning. We anticipate a future where large language models will work more cohesively with knowledge graphs, bringing about more innovative solutions to real-world problems.</p>"}]}